@inproceedings{10.1145/3710848.3710857,
author = {Di, Zhanyuan and Wang, Leping and Ren, Ziyi and Shao, En and Zhao, Jie and Feng, Siyuan and Tao, Dingwen and Tan, Guangming and Sun, Ninghui},
title = {Magneto: Accelerating Parallel Structures in DNNs via Co-Optimization of Operators},
year = {2025},
isbn = {9798400714436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3710848.3710857},
doi = {10.1145/3710848.3710857},
abstract = {Deep neural networks (DNNs) increasingly rely on parallel structures to enhance performance and efficiency. However, existing machine learning compilers (MLCs) face challenges in optimizing these structures due to limited parallel fusion scopes and insufficient consideration of intra-operator information. This paper introduces Magneto, a novel framework designed to accelerate parallel structures in DNNs through the co-optimization of parallel operators. By expanding the scope of parallel operator fusion and introducing a dedicated co-tuning algorithm, Magneto unlocks new opportunities for co-optimization. Experimental results demonstrate that Magneto outperforms NVIDIA TensorRT and AMD MIGraphX, achieving speedups of 3.02\texttimes{} and 4.19\texttimes{}, respectively.},
booktitle = {Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {563â€“565},
numpages = {3},
keywords = {DNN, GPU, Inference},
location = {Las Vegas, NV, USA},
series = {PPoPP '25}
}